# -*- coding: utf-8 -*-
"""
Created on Wed Dec 17 14:41:47 2025

@author: navar
"""

# -*- coding: utf-8 -*-
"""
OBS-PCA heart artifact removal per ESG channel using MANUAL R-peaks.
- Input raw: BrainVision .vhdr (original can be 10 kHz)
- Step 1: resample COPY of full raw to 1000 Hz (ALL channels)
- Step 2: load rpeaks_manual TSVs (generated at 1000 Hz)
- Step 3: for each ESG channel separately:
          epoch each artifact around R-peak with window [-0.5*medianRR, +0.5*medianRR]
          PCA on (artifacts x time)
          OBS = mean artifact + first 4 PCs
          fit OBS to each artifact epoch, subtract fitted artifact
          reconstruct continuous channel by subtracting within the epochs (overlaps averaged)
- Output: cleaned/resampled raw saved as FIF
"""

from pathlib import Path
import re
import numpy as np
import pandas as pd
import mne
from sklearn.decomposition import PCA

# ============================================================
# CONFIG
# ============================================================

SUBJECT_DIR = Path(r"C:\Users\navar\Desktop\sub-P01\sub-P01")
SUBJECT_LABEL = "sub-P01"

# Input raws
RAW_EXTS = (".vhdr",)
RAW_NAME_MUST_CONTAIN = None  # e.g. "NoStimArtifact" or None

# Manual rpeaks (these TSVs are at 1000 Hz)
RPEAKS_DIR = SUBJECT_DIR / "rpeaks_manual"  # <- cambia si tu carpeta es "rpeaks_manual"
RPEAKS_TSV_GLOB = f"{SUBJECT_LABEL}_Run*_rpeaks_manual.tsv"  # ajusta si tu naming difiere

# Resample target
TARGET_FS = 1000.0

# ESG channels (as you provided)
ESG_CERV = [
    "Iz", "SC1", "S3", "S4", "S5", "S6", "S7", "S8", "S9",
    "S11", "S12", "S13", "S14", "S15", "S16", "S17", "S18", "S19",
    "SC6", "AC", "TH6"
]

# OBS/PCA
N_PCS = 4           # first 4 components
RIDGE_LAMBDA = 0.0  # 0 = ordinary least squares; try 1e-6 if you want stabilization

# Output
OUT_DIR = SUBJECT_DIR / "esg_obs_clean_1k"
OUT_DIR.mkdir(exist_ok=True)
SAVE_TEMPLATES = False  # if True, saves per-channel basis templates as .npz (can get big)

RAW_NAME_MUST_CONTAIN = "NoStimArtifact"

# ============================================================
# Helpers
# ============================================================

def parse_run_number(name: str):
    m = re.search(r"(?:run[-_]?|Run[-_]?)(\d+)", name)
    return int(m.group(1)) if m else None

def find_raw_files(subject_dir: Path):
    files = []
    for ext in RAW_EXTS:
        files.extend(subject_dir.rglob(f"*{ext}"))
    files = sorted(files)
    if RAW_NAME_MUST_CONTAIN:
        files = [f for f in files if RAW_NAME_MUST_CONTAIN in f.name]
    return files

def read_raw_brainvision(path: Path) -> mne.io.BaseRaw:
    return mne.io.read_raw_brainvision(path, preload=True, verbose="ERROR")

def load_manual_rpeaks_1k(tsv_path: Path) -> np.ndarray:
    """Return peaks in samples @ 1000 Hz."""
    df = pd.read_csv(tsv_path, sep="\t")
    if "sample_1k" in df.columns:
        return df["sample_1k"].to_numpy(dtype=np.int64)
    if "time_sec" in df.columns:
        return np.rint(df["time_sec"].to_numpy(dtype=float) * TARGET_FS).astype(np.int64)
    raise RuntimeError(f"TSV missing 'sample_1k' or 'time_sec': {tsv_path}")

def median_rr_sec_from_peaks(peaks_samp: np.ndarray, sfreq: float) -> float | None:
    peaks_samp = np.asarray(peaks_samp, dtype=np.int64)
    peaks_samp = np.unique(peaks_samp[(peaks_samp >= 0)])
    if peaks_samp.size < 3:
        return None
    rr = np.diff(peaks_samp) / sfreq
    rr = rr[np.isfinite(rr) & (rr > 0)]
    if rr.size == 0:
        return None
    return float(np.median(rr))

def extract_epochs(x: np.ndarray, peaks: np.ndarray, sfreq: float, tmin: float, tmax: float):
    """
    x: (n_samples,)
    peaks: sample indices
    returns:
      E: (n_epochs, n_times)
      used_peaks: (n_epochs,)
      offsets: (n_times,) integer offsets relative to peak
    """
    n = x.size
    a_off = int(np.round(tmin * sfreq))
    b_off = int(np.round(tmax * sfreq))
    if b_off <= a_off:
        raise ValueError("Bad epoch window: tmax must be > tmin")

    offsets = np.arange(a_off, b_off, dtype=np.int64)
    epochs = []
    used = []

    for p in peaks.astype(np.int64):
        idx = p + offsets
        if idx[0] < 0 or idx[-1] >= n:
            continue
        epochs.append(x[idx])
        used.append(p)

    if not epochs:
        return None, None, None

    return np.vstack(epochs), np.asarray(used, dtype=np.int64), offsets

def obs_pca_clean_channel(x: np.ndarray, peaks: np.ndarray, sfreq: float, median_rr: float,
                          n_pcs: int = 4, ridge_lambda: float = 0.0):
    """
    Clean a single channel using OBS = mean + first n_pcs PCA components.

    Returns:
      x_clean (n_samples,)
      info dict (basis, offsets, etc.) or None if not enough epochs
    """
    tmin = -0.5 * median_rr
    tmax = +0.5 * median_rr

    E, used_peaks, offsets = extract_epochs(x, peaks, sfreq, tmin, tmax)
    if E is None:
        return x.copy(), None

    # Need enough epochs to estimate PCA + mean robustly
    if E.shape[0] < max(10, n_pcs + 2):
        return x.copy(), None

    # Mean artifact
    mean_art = E.mean(axis=0, keepdims=True)         # (1, n_times)
    Xc = E - mean_art                                # centered by mean artifact

    # PCA on (epochs x time)
    n_comp = min(n_pcs, Xc.shape[0], Xc.shape[1])
    pca = PCA(n_components=n_comp)
    pca.fit(Xc)
    pcs = pca.components_                             # (n_comp, n_times)

    # OBS basis B: (n_basis, n_times)
    B = np.vstack([mean_art, pcs])                    # (1+n_comp, n_times)
    n_basis = B.shape[0]

    # Fit per-epoch using least squares in basis-space:
    # beta = y * B^T * inv(BB^T + λI)
    BBt = B @ B.T                                     # (n_basis, n_basis)
    if ridge_lambda > 0:
        BBt = BBt + ridge_lambda * np.eye(n_basis)
    inv = np.linalg.pinv(BBt)
    Bt = B.T                                          # (n_times, n_basis)

    # Artifact estimate for each epoch
    betas = (E @ Bt) @ inv                            # (n_epochs, n_basis)
    Ehat = betas @ B                                  # (n_epochs, n_times)

    # Reconstruct continuous subtraction (average overlaps)
    n = x.size
    sub = np.zeros(n, dtype=float)
    wgt = np.zeros(n, dtype=float)

    for i, p in enumerate(used_peaks):
        idx = p + offsets
        sub[idx] += Ehat[i]
        wgt[idx] += 1.0

    x_clean = x.astype(float).copy()
    mask = wgt > 0
    x_clean[mask] = x_clean[mask] - (sub[mask] / wgt[mask])

    info = {
        "tmin": tmin,
        "tmax": tmax,
        "median_rr": median_rr,
        "n_epochs_used": int(len(used_peaks)),
        "n_basis": int(n_basis),
        "explained_variance_ratio": pca.explained_variance_ratio_.copy(),
        "basis_B": B,          # rows: [mean, pc1..]
        "offsets": offsets
    }
    return x_clean, info

# ============================================================
# MAIN
# ============================================================

def main():
    # 1) Map run -> raw vhdr
    raw_files = find_raw_files(SUBJECT_DIR)
    run_to_raw = {}
    for rp in raw_files:
        r = parse_run_number(rp.name)
        if r is None:
            continue
        run_to_raw.setdefault(r, rp)

    if not run_to_raw:
        raise SystemExit(f"No .vhdr files found in {SUBJECT_DIR}")

    # 2) TSV list
    tsv_files = sorted(RPEAKS_DIR.glob(RPEAKS_TSV_GLOB))
    if not tsv_files:
        raise SystemExit(f"No TSVs found in {RPEAKS_DIR} with glob {RPEAKS_TSV_GLOB}")

    summary_rows = []

    for tsv in tsv_files:
        run = parse_run_number(tsv.name)
        if run is None:
            print(f"[SKIP] cannot parse run from TSV: {tsv.name}")
            continue
        if run not in run_to_raw:
            print(f"[SKIP] no VHDR for run {run:02d} (TSV={tsv.name})")
            continue

        raw_path = run_to_raw[run]
        print("\n============================================================")
        print(f"RUN {run:02d}")
        print(f"RAW: {raw_path}")
        print(f"TSV: {tsv}")

        # Load peaks (samples @ 1k)
        peaks_1k = load_manual_rpeaks_1k(tsv)
        peaks_1k = np.unique(peaks_1k[peaks_1k >= 0]).astype(np.int64)
        print(f"[INFO] peaks in TSV (1k): {len(peaks_1k)}")

        # Read raw @ 10k (or whatever) and resample COPY to 1k (ALL channels)
        raw = read_raw_brainvision(raw_path)
        raw_1k = raw.copy().resample(TARGET_FS, npad="auto")
        fs = float(raw_1k.info["sfreq"])
        assert abs(fs - TARGET_FS) < 1e-6, "Resample to 1000 Hz failed"

        n_samples = raw_1k.n_times
        # keep only in-range peaks
        inb = (peaks_1k >= 0) & (peaks_1k < n_samples)
        if np.sum(~inb) > 0:
            print(f"[WARN] {np.sum(~inb)} peaks outside range after resample; dropping them.")
        peaks_1k = peaks_1k[inb]

        med_rr = median_rr_sec_from_peaks(peaks_1k, fs)
        if med_rr is None:
            print("[SKIP] cannot compute median RR")
            continue

        win = 0.5 * med_rr
        print(f"[INFO] median RR = {med_rr:.4f}s | epoch window = ±{win:.4f}s")

        # Decide ESG channels present
        esg_present = [ch for ch in ESG_CERV if ch in raw_1k.ch_names]
        missing = [ch for ch in ESG_CERV if ch not in raw_1k.ch_names]
        print(f"[INFO] ESG channels found: {len(esg_present)} | missing: {len(missing)}")
        if len(esg_present) == 0:
            print("[SKIP] none of ESG_CERV channels present in this run")
            continue

        # Clean only ESG channels (others untouched in raw_1k)
        template_dump = {}

        for ch in esg_present:
            x = raw_1k.get_data(picks=[ch])[0]
            x_clean, info = obs_pca_clean_channel(
                x=x,
                peaks=peaks_1k,
                sfreq=fs,
                median_rr=med_rr,
                n_pcs=N_PCS,
                ridge_lambda=RIDGE_LAMBDA
            )
            raw_1k._data[raw_1k.ch_names.index(ch), :] = x_clean

            if info is None:
                print(f"  [WARN] {ch}: not enough usable epochs for PCA/OBS -> left unchanged")
            else:
                print(f"  [OK] {ch}: epochs={info['n_epochs_used']} basis={info['n_basis']}")

            if SAVE_TEMPLATES and info is not None:
                template_dump[ch] = {
                    "basis_B": info["basis_B"],
                    "offsets": info["offsets"],
                    "explained_variance_ratio": info["explained_variance_ratio"],
                    "median_rr": info["median_rr"],
                    "tmin": info["tmin"],
                    "tmax": info["tmax"],
                }

        # Save cleaned/resampled raw (FIF recommended)
        out_fif = OUT_DIR / f"{SUBJECT_LABEL}_Run{run:02d}_resamp1k_ESGobsClean_raw.fif"
        raw_1k.save(out_fif, overwrite=True)
        print(f"[SAVED] {out_fif}")

        if SAVE_TEMPLATES and template_dump:
            out_npz = OUT_DIR / f"{SUBJECT_LABEL}_Run{run:02d}_OBS_templates.npz"
            # store as object arrays
            np.savez_compressed(out_npz, **{k: v for k, v in template_dump.items()})
            print(f"[SAVED] {out_npz}")

        summary_rows.append({
            "run": run,
            "raw_vhdr": str(raw_path),
            "tsv_manual": str(tsv),
            "orig_sfreq_hz": float(raw.info["sfreq"]),
            "proc_sfreq_hz": fs,
            "n_samples_1k": int(n_samples),
            "n_peaks_used": int(len(peaks_1k)),
            "median_rr_sec": float(med_rr),
            "epoch_halfwin_sec": float(win),
            "n_esg_channels_found": int(len(esg_present)),
            "n_esg_channels_missing": int(len(missing)),
            "out_fif": str(out_fif),
            "n_pcs": int(N_PCS),
            "ridge_lambda": float(RIDGE_LAMBDA),
        })

    if summary_rows:
        df = pd.DataFrame(summary_rows).sort_values("run")
        out_csv = OUT_DIR / f"{SUBJECT_LABEL}_OBS_PCA_summary.csv"
        df.to_csv(out_csv, index=False, encoding="utf-8")
        print(f"\n[SUMMARY] {out_csv}")

if __name__ == "__main__":
    main()
